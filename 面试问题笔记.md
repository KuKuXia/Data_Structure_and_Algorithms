# 面试问题错题集

## 生成式和判别式模型

### 常见的判别式模型有：

- Logistic regression（logistical 回归）
- Linear discriminant analysis（线性判别分析）
- Supportvector machines（支持向量机）
- Boosting（集成学习）
- Conditional random fields（条件随机场）
- Linear regression（线性回归）
- Neural networks（神经网络）

### 常见的生成式模型有:
- Gaussian mixture model and othertypes of mixture model（高斯混合及其他类型混合模型）
- Hidden Markov model（隐马尔可夫）
- NaiveBayes（朴素贝叶斯）
- AODE（平均单依赖估计）
- Latent Dirichlet allocation（LDA主题模型）
- Restricted Boltzmann Machine（限制波兹曼机）
  **生成式模型是根据概率乘出结果，而判别式模型是给出输入，计算出结果**

### 时间序列预测模型

- AR模型是一种线性预测，即已知N个数据，可由模型推出第N点前面或后面的数据（设推出P点），所以其本质类似于插值。
- MA模型(moving average model)滑动平均模型，其中使用趋势移动平均法建立直线趋势的预测模型。
  ARMA模型(auto regressive moving average model)自回归滑动平均模型，模型参量法高分辨率谱分析方法之一。这种方法是研究平稳随机过程有理谱的典型方法。它比AR模型法与MA模型法有较精确的谱估计及较优良的谱分辨率性能，但其参数估算比较繁琐。
- GARCH模型称为广义ARCH模型，是ARCH模型的拓展，由Bollerslev(1986)发展起来的。它是ARCH模型的推广。GARCH(p,0)模型，相当于ARCH(p)模型。GARCH模型是一个专门针对金融数据所量体订做的回归模型，除去和普通回归模型相同的之处，GARCH对误差的方差进行了进一步的建模。特别适用于波动性的分析和预测，这样的分析对投资者的决策能起到非常重要的指导性作用，其意义很多时候超过了对数值本身的分析和预测

### 常用特征选择算法

常采用特征选择方法。常见的六种特征选择方法：

1. DF(Document Frequency) 文档频率
   DF:统计特征词出现的文档数量，用来衡量某个特征词的重要性
2. MI(Mutual Information) 互信息法
   互信息法用于衡量特征词与文档类别直接的信息量。
   如果某个特征词的频率很低，那么互信息得分就会很大，因此互信息法倾向"低频"的特征词。
   相对的词频很高的词，得分就会变低，如果这词携带了很高的信息量，互信息法就会变得低效。
3. (Information Gain) 信息增益法
   通过某个特征词的缺失与存在的两种情况下，语料中前后信息的增加，衡量某个特征词的重要性。
4. CHI(Chi-square) 卡方检验法
   利用了统计学中的"假设检验"的基本思想：首先假设特征词与类别直接是不相关的
   如果利用CHI分布计算出的检验值偏离阈值越大，那么更有信心否定原假设，接受原假设的备则假设：特征词与类别有着很高的关联度。
5. WLLR(Weighted Log Likelihood Ration)加权对数似然
6. WFO（Weighted Frequency and Odds）加权频率和可能性

### CRF与HMM和MEMM模型

1. CRF 的优点：特征灵活，可以容纳较多的上下文信息，能够做到全局最优CRF 的缺点：速度慢
2. CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息。特征设计灵活（与ME一样） ————与HMM比较
3. 同时，由于CRF计算全局最优输出节点的条件概率，它还克服了最大熵马尔可夫模型标记偏置（Label-bias）的缺点。 ­­————与MEMM比较
4. CRF是在给定需要标记的观察序列的条件下，使用维特比算法，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。————与ME比较

### R-square与Adjusted R-squared

1. 线性回归中，R-squared描述的是输入变量（特征）对输出变量的解释程度。在单变量线性回归中R-squared 越大，说明拟合程度越好；而在多变量的情况下，无论增加的特征与输出是否存在关系（即是否重要），R-squared 要么保持不变，要么增加。故本题中可能的选项只有A。（本题中增加一个特征后至少有两个特征，所欲属于多特征范畴）

2. 多变量线性回归使用adjusted R-squared评估模型效果。并且增加一个特征变量，如果这个特征有意义，Adjusted R-Square 就会增大，若这个特征是冗余特征，Adjusted R-Squared 就会减小。

3. 单变量线性回归中，R-squared和adjusted R-squared是一致的，即重要特征使R-squared增大，冗余特征使R-squared减小。

### 数据清理中，处理缺失值的方法

据清理中，处理缺失值的方法有两种：

#### 删除法

1. 删除观察样法
2. 删除变量：当某个变量缺失值较多且对研究目标影响不大时，可以将整个变量整体删除
3. 使用完整原始数据分析：当数据存在较多缺失而其原始数据完整时，可以使用原始数据替代现有数据进行分析
4. 改变权重：当删除缺失数据会改变数据结构时，通过对完整数据按照不同的权重进行加权，可以降低删除缺失数据带来的偏差

#### 查补法

1. 均值插补
2. 回归插补
3. 抽样填补等

## xgboost和lr，对比其他分类算法的场景优势。

因为项目要需要更高的效率，所以要用xgboost和lr。

### Xgboost

优缺点：1）在寻找最佳分割点时，考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。2）xgboost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率，paper提到50倍。3）特征列排序后以块的形式存储在内存中，在迭代中可以重复使用；虽然boosting算法迭代必须串行，但是在处理每个特征列时可以做到并行。4）按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致cache miss，降低算法效率。paper中提到，可先将数据收集到线程内部的buffer，然后再计算，提高算法的效率。5）xgboost 还考虑了当数据量比较大，内存不够时怎么有效的使用磁盘，主要是结合多线程、数据压缩、分片的方法，尽可能的提高算法的效率。

适用场景：分类回归问题都可以。

### Lr

优点：实现简单，广泛的应用于工业问题上；分类时计算量非常小，速度很快，存储资源低；便利的观测样本概率分数；对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题。

缺点：当特征空间很大时，逻辑回归的性能不是很好；容易欠拟合，一般准确度不太高

不能很好地处理大量多类特征或变量；只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；对于非线性特征，需要进行转换。

适用场景：LR同样是很多分类算法的基础组件，它的好处是输出值自然地落在0到1之间，并且有概率意义。因为它本质上是一个线性的分类器，所以处理不好特征之间相关的情况。虽然效果一般，却胜在模型清晰，背后的概率学经得住推敲。它拟合出来的参数就代表了每一个特征(feature)对结果的影响。也是一个理解数据的好工具。



## ISODATA算法比kmeans有哪些改进

K-means算法通常适合于分类数目已知的聚类，而ISODATA算法则更加灵活；2）从算法角度看，ISODATA算法与K-means算法相似，聚类中心都是通过样本均值的迭代运算来决定的；3）ISODATA算法加入了一些试探步骤，并且可以结合成人机交互的结构，使其能利用中间结果所取得的经验更好地进行分类。



## 描述一个算法项目从Kickoff到落地的全过程

撰写项目指南，分析项目需求，制定项目方案，确定技术路线，开始实施，测试，反馈，改进，上线运行。

## 如何处理数据中经常存在的数据不平衡的问题。

针对不平衡数据集的问题，可以采用以下几种方法：
从数据的角度：
重采样，　又分为欠采样和过采样
K-fold交叉验证
对于正负样本极不均匀的问题，采用异常检测或者一分类问题
组合不同的重采样数据集。建立n个模型，每个模型使用稀有类别的所有样本和丰富类别的n个不同样本（比例为１：１）
用不同的比例重新采样
对丰富类别进行聚类处理，并使用类中心作为样本与稀有类样本组合成数据集训练
从模型的角度：
１．　设计针对不平衡数据集的模型，如果设计的模型本身就适用于不平衡数据，则不需要重新采样数据，XGBoost，就是其中的典型代表。
２．　通过设计一个代价函数来惩罚稀有类别的错误分类而不是分类丰富类别，可以设计出许多自然汉化为稀有类别的模型。例如：调整SVM以惩罚稀有类别的错误分类。    

## IP报文经过一个路由器改变哪些字段

源和目的地的IP地址，源和目的地的MAC地址以及TTL值

　

## FastText 和Glovec原理

FastText是将句子中的每个词通过一个lookup层映射成词向量,对词向量叠加取平均作为句子的向量,然后直接用线性分类器进行分类,FastText中没有非线性的隐藏层,结构相对简单而且模型训练的更快。
Glovec融合了矩阵分解和全局统计信息的优势,统计语料库的词-词之间的共现矩阵,加快模型的训练速度而且又可以控制词的相对权重。



## Word2Vec中skip-gram是什么,Negative Sampling怎么做

Word2Vec通过学习文本然后用词向量的方式表征词的语义信息,然后使得语义相似的单词在嵌入式空间中的距离很近。而在Word2Vec模型中有Skip-Gram和CBOW两种模式,Skip-Gram是给定输入单词来预测上下文,而CBOW与之相反,是给定上下文来预测输入单词。Negative Sampling是对于给定的词,并生成其负采样词集合的一种策略,已知有一个词,这个词可以看做一个正例,而它的上下文词集可以看做是负例,但是负例的样本太多,而在语料库中,各个词出现的频率是不一样的,所以在采样时可以要求高频词选中的概率较大,低频词选中的概率较小,这样就转化为一个带权采样问题,大幅度提高了模型的性能。



## 模拟退火和蚁群对比

模拟退火算法:退火是一个物理过程，粒子可以从高能状态转换为低能状态，而从低能转换为高能则有一定的随机性，且温度越低越难从低能转换为高能。就是在物体冷却的过程中，物体内部的粒子并不是同时冷却下来的。因为在外围粒子降温的时候，里边的粒子还会将热量传给外围粒子，就可能导致局部粒子温度上升，当然，最终物体的温度还是会下降到环境温度的。
先给出一种求最优解的方法，在寻找一个最优解的过程中采取分段检索的方式，在可行解中随机找出来一个，然后在这个解附近进行检索，找到更加优化的解，放弃原来的，在新得到的解附近继续检索，当无法继续找到一个更优解的时候就认为算法收敛，最终得到的是一个局部最优解。

蚁群算法:很显然，就是模仿蚂蚁寻找食物而发明的算法，主要用途就是寻找最佳路径。先讲一下蚂蚁是怎么寻找食物，并且在寻找到了食物后怎么逐渐确定最佳路径的。

事实上，蚂蚁的目光很短，它只会检索它附近的很小一部分空间，但是它在寻路过程中不会去重复它留下信息素的空间，也就是它不会往回走。当一群蚂蚁遇到障碍物了之后，它们会随机分为两拨，一波往左一波往右，但是因为环境会挥发掉它们的信息素，于是，较短的路留下的信息素多，而较长的路因为挥发较多，也就留下得少了。接下来，蚁群就会趋向于行走信息素较多的路径，于是大部分蚂蚁就走了较短的路了。但是蚁群中又有一些特别的蚂蚁，它们并不会走大家走的路，而是以一定概率寻找新路，如果新路更短，信息素挥发更少，渐渐得蚁群就会走向更短的路径。

以上就是蚂蚁寻路的具体过程。把这个过程映射到计算机算法上，计算机在单次迭代过程中，会在路径的节点上留下信息素（可以使用数据变量来表示）。每次迭代都做信息素蒸发处理，多次迭代后聚集信息素较多的路径即可认为是较优路径。

## 贪婪算法

贪婪算法(贪心算法)是指在对问题进行求解时，在每一步选择中都采取最好或者最优(即最有利)的选择，从而希望能够导致结果是最好或者最优的算法。贪婪算法所得到的结果往往不是最优的结果(有时候会是最优解)，但是都是相对近似(接近)最优解的结果。贪婪算法并没有固定的算法解决框架，算法的关键是贪婪策略的选择，根据不同的问题选择不同的策略。
必须注意的是策略的选择必须具备无后效性，即某个状态的选择不会影响到之前的状态，只与当前状态有关，所以对采用的贪婪的策略一定要仔细分析其是否满足无后效性。

其基本的解题思路为：1)建立数学模型来描述问题;2)把求解的问题分成若干个子问题;3)对每一子问题求解，得到子问题的局部最优解;4)把子问题对应的局部最优解合成原来整个问题的一个近似最优解。

### Note

1. 朴素贝叶斯的条件就是每隔变量相互独立
2. L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso（L1）在特征选择时候非常有用，而Ridge（L2）就只是一种规则化而已。
3. Pearson相关系数只能衡量线性相关性，但无法衡量非线性关系。如y=x^2，x和y有很强的非线性关系。
4. boostrap是提鞋自举的意思(武侠小说作者所说的左脚踩右脚腾空而起). 它的过程是对样本(而不是特征)进行有放回的抽样, 抽样次数等同于样本总数. 这个随机抽样过程决定了最终抽样出来的样本, 去除重复之后, 占据原有样本的1/e比例. 增加树的深度, 会导致所有节点不断分裂, 直到叶子节点是纯的为止. 所以, 增加深度, 会延长训练时间.
   决策树没有学习率参数可以调. (不像集成学习和其它有步长的学习方法)
   决策树只有一棵树, 不是随机森林。
5. 特征之间的相关性系数不会因为特征加或减去一个数而改变。
6. 当数据有一个 0 均值向量时，PCA 有与 SVD 一样的投射，否则在使用 SVD 之前，你必须将数据均值归 0。
7. 由信息量公式 I(X) = - log[p(X)] 知，概率越大，信息量越小
8. 主成分分析是特征转换算法（特征抽取），而不是特征选择
9. 语言模型的参数估计经常使用MLE（最大似然估计）。面临的一个问题是没有出现的项概率为0，这样会导致语言模型的效果不好。为了解决这个问题，需要使用拉普拉斯平滑假设，将分子和分母各加上一个常数项。
10. SVM的决策边界只会被支持向量影响，跟其他点无关。
11. SVM的效率依赖于核函数的选择、核参数、软间隔参数
12. 对于SVM，γ 越大，模型对训练数据的拟合效果越好，当 γ 很大时，模型会对数据过拟合，即：分类超平面波动较大，几乎是贴着训练数据的
13. C >0称为惩罚参数，是调和二者的系数，C值大时对误差分类的惩罚增大，当C越大，趋近无穷的时候，表示不允许分类误差的存在，margin越小，容易过拟合。C值小时对误差分类的惩罚减小,当C趋于0时，表示我们不再关注分类是否正确，只要求margin越大，容易欠拟合。

14. 进程通信的几大方法中三个，分别是管道( pipe )、套接字( socket )、共享内存( shared memory)。
15. t-SNE代表t分布随机相邻嵌入，它考虑最近的邻居来减少数据。
16. SNE（随机邻域嵌入）将数据点之间高维的欧氏距离转换为表示相似度的条件概率，t-SNE重点解决了两个问题，一个是SNE的不对称问题（用联合概率分布替代条件概率分布），另一个是不同类别之间簇的拥挤问题（引入t分布）。机器学习可以总结为学习一个函数模型，它将输入变量X映射为输出变量Y，算法从训练数据中学习这个映射函数，如果事先假设了模型参数和目标函数，那么这样的机器学习算法就是在学习一种参数映射，所以t-SNE学习非参数映射。
17. 降维过程中，两点的相似性的条件概率必须相等，因为点之间的相似性必须在高维和低维中保持不变，以使它们成为完美的表示。
18. LDA在样本分类信息依赖方差而不是均值的时候，降维效果不好；LDA是有监督学习，它既可以用作数据降维，又可以用于分类，但要保证不同类别数据的投影中心尽可能远；有辨识的信息即分类依据，如果有辨识的信息不是平均值，那么就无法保证投影后的异类数据点中心尽可能远，LDA就会失败。
19. LDA最多产生c-1个判别向量。
20. AIC信息准则即Akaike information criterion，是衡量统计模型拟合优良性的一种标准，由于它为日本统计学家赤池弘次创立和发展的，因此又称赤池信息量准则。考虑到AIC=2k-2In(L) ，所以一般而言，当模型复杂度提高（k增大）时，似然函数L也会增大，从而使AIC变小，但是k过大时，似然函数增速减缓，导致AIC增大，模型过于复杂容易造成过拟合现象。目标是选取AIC最小的模型，AIC不仅要提高模型拟合度（极大似然），而且引入了惩罚项，使模型参数尽可能少，有助于降低过拟合的可能性。综上，我们一般选择逻辑回归中最少的AIC作为最佳模型。
21. 输出特征图尺寸=（N-F+2P）/ S + 1，其中N为输入图像的宽和高，F为卷积核的宽和高，P为填充值padding，S为卷积核滑动步长stride。
22. 线性回归有变化的误差项，在误差项中，非恒定方差的存在导致了异方差性。一般来说，非恒定方差的出现时因为异常值或极端杠杆值的存在。
23. 一般将残差想作垂直坐标，正交坐标在PCA的例子中很有用
24. 线性回归中，大特征值=⇒小相关系数=⇒更少lasso penalty =⇒更可能被保留
25. Ridge回归在最终模型中用到了所有自变量，然而Lasso回归可被用于特征值选择，因为相关系数可以为零
26. 异常值是指样本中的个别值明显偏离其余观测值，也叫离群值。目前人们对异常值的判别与剔除主要采用物理判别法和统计判别法两种方法。所谓物理判别法就是根据人们对客观事物已有的认识，判别由于外界干扰、人为误差等原因造成实测数据值偏离正常结果，在实验过程中随时判断，随时剔除。统计判别法是给定一个置信概率，并确定一个置信限，凡超过此限的误差，就认为它不属于随机误差范围，将其视为异常值剔除。当物理识别不易判断时，一般采用统计识别法。该题中，所给的信息量过少，无法肯定一定是异常值。
27. 使用 L1 正则化的模型叫做 Lasson 回归，使用 L2 正则化的模型叫做 Ridge 回归（岭回归）。L1正则可以保证模型的稀疏性，也就是某些参数等于0；L2正则可以保证模型的稳定性，也就是参数的值不会太大或太小。L1正则和L2正则都可以减小模型复杂度，防止过拟合。
    1. Lambda很大表示模型比较简单，这种情况下模型预测值与实际值差距较大，即偏差大，而预测值的变化范围较小，即方差小。
    2. 当lambda为0时我们得到了最小的最小二乘解；当lambda趋近无穷时，会得到非常小、趋近0的相关系数。
28. 预测值与残差之间不应该存在任何函数关系，若存在函数关系，表明模型拟合的效果并不很好。
29. Lasso不允许闭式解，L1-penalty使解为非线性的，所以需要近似解。
30. 逻辑回归的输出是因变量Y取值的概率，用于分类问题，而不是对因变量的取值进行预测，而线性回归的输出是模型对因变量Y的取值进行预测和估计。
31. 决策树还可以用在数据中的聚类分析，但是聚类常常生成自然集群，并且不依赖于任何目标函数。x例如 基于最大似然（ML）准则的决策树聚类 以及 基于最小描述长度（MDL）的决策树聚类
32. 使用信息增益作为决策树节点属性选择的标准，由于信息增益在类别值多的属性上计算结果大于类别值少的属性上计算结果，这将导致决策树算法偏向选择具有较多分枝的属性。