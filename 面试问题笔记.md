# 面试问题错题集

## 生成式和判别式模型

### 常见的判别式模型有：

- Logistic regression（logistical 回归）
- Linear discriminant analysis（线性判别分析）
- Supportvector machines（支持向量机）
- Boosting（集成学习）
- Conditional random fields（条件随机场）
- Linear regression（线性回归）
- Neural networks（神经网络）

### 常见的生成式模型有:
- Gaussian mixture model and othertypes of mixture model（高斯混合及其他类型混合模型）
- Hidden Markov model（隐马尔可夫）
- NaiveBayes（朴素贝叶斯）
- AODE（平均单依赖估计）
- Latent Dirichlet allocation（LDA主题模型）
- Restricted Boltzmann Machine（限制波兹曼机）
  **生成式模型是根据概率乘出结果，而判别式模型是给出输入，计算出结果**

### 时间序列预测模型

- AR模型是一种线性预测，即已知N个数据，可由模型推出第N点前面或后面的数据（设推出P点），所以其本质类似于插值。
- MA模型(moving average model)滑动平均模型，其中使用趋势移动平均法建立直线趋势的预测模型。
  ARMA模型(auto regressive moving average model)自回归滑动平均模型，模型参量法高分辨率谱分析方法之一。这种方法是研究平稳随机过程有理谱的典型方法。它比AR模型法与MA模型法有较精确的谱估计及较优良的谱分辨率性能，但其参数估算比较繁琐。
- GARCH模型称为广义ARCH模型，是ARCH模型的拓展，由Bollerslev(1986)发展起来的。它是ARCH模型的推广。GARCH(p,0)模型，相当于ARCH(p)模型。GARCH模型是一个专门针对金融数据所量体订做的回归模型，除去和普通回归模型相同的之处，GARCH对误差的方差进行了进一步的建模。特别适用于波动性的分析和预测，这样的分析对投资者的决策能起到非常重要的指导性作用，其意义很多时候超过了对数值本身的分析和预测

### 常用特征选择算法

常采用特征选择方法。常见的六种特征选择方法：

1. DF(Document Frequency) 文档频率
   DF:统计特征词出现的文档数量，用来衡量某个特征词的重要性
2. MI(Mutual Information) 互信息法
   互信息法用于衡量特征词与文档类别直接的信息量。
   如果某个特征词的频率很低，那么互信息得分就会很大，因此互信息法倾向"低频"的特征词。
   相对的词频很高的词，得分就会变低，如果这词携带了很高的信息量，互信息法就会变得低效。
3. (Information Gain) 信息增益法
   通过某个特征词的缺失与存在的两种情况下，语料中前后信息的增加，衡量某个特征词的重要性。
4. CHI(Chi-square) 卡方检验法
   利用了统计学中的"假设检验"的基本思想：首先假设特征词与类别直接是不相关的
   如果利用CHI分布计算出的检验值偏离阈值越大，那么更有信心否定原假设，接受原假设的备则假设：特征词与类别有着很高的关联度。
5. WLLR(Weighted Log Likelihood Ration)加权对数似然
6. WFO（Weighted Frequency and Odds）加权频率和可能性

### CRF与HMM和MEMM模型

1. CRF 的优点：特征灵活，可以容纳较多的上下文信息，能够做到全局最优CRF 的缺点：速度慢
2. CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息。特征设计灵活（与ME一样） ————与HMM比较
3. 同时，由于CRF计算全局最优输出节点的条件概率，它还克服了最大熵马尔可夫模型标记偏置（Label-bias）的缺点。 ­­————与MEMM比较
4. CRF是在给定需要标记的观察序列的条件下，使用维特比算法，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。————与ME比较

### R-square与Adjusted R-squared

1. 线性回归中，R-squared描述的是输入变量（特征）对输出变量的解释程度。在单变量线性回归中R-squared 越大，说明拟合程度越好；而在多变量的情况下，无论增加的特征与输出是否存在关系（即是否重要），R-squared 要么保持不变，要么增加。故本题中可能的选项只有A。（本题中增加一个特征后至少有两个特征，所欲属于多特征范畴）

2. 多变量线性回归使用adjusted R-squared评估模型效果。并且增加一个特征变量，如果这个特征有意义，Adjusted R-Square 就会增大，若这个特征是冗余特征，Adjusted R-Squared 就会减小。

3. 单变量线性回归中，R-squared和adjusted R-squared是一致的，即重要特征使R-squared增大，冗余特征使R-squared减小。

### 数据清理中，处理缺失值的方法

据清理中，处理缺失值的方法有两种：

#### 删除法

1. 删除观察样法
2. 删除变量：当某个变量缺失值较多且对研究目标影响不大时，可以将整个变量整体删除
3. 使用完整原始数据分析：当数据存在较多缺失而其原始数据完整时，可以使用原始数据替代现有数据进行分析
4. 改变权重：当删除缺失数据会改变数据结构时，通过对完整数据按照不同的权重进行加权，可以降低删除缺失数据带来的偏差

#### 查补法

1. 均值插补
2. 回归插补
3. 抽样填补等

### Note

1. 朴素贝叶斯的条件就是每隔变量相互独立
2. L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso（L1）在特征选择时候非常有用，而Ridge（L2）就只是一种规则化而已。
3. Pearson相关系数只能衡量线性相关性，但无法衡量非线性关系。如y=x^2，x和y有很强的非线性关系。
4. boostrap是提鞋自举的意思(武侠小说作者所说的左脚踩右脚腾空而起). 它的过程是对样本(而不是特征)进行有放回的抽样, 抽样次数等同于样本总数. 这个随机抽样过程决定了最终抽样出来的样本, 去除重复之后, 占据原有样本的1/e比例. 增加树的深度, 会导致所有节点不断分裂, 直到叶子节点是纯的为止. 所以, 增加深度, 会延长训练时间.
   决策树没有学习率参数可以调. (不像集成学习和其它有步长的学习方法)
   决策树只有一棵树, 不是随机森林。
5. 特征之间的相关性系数不会因为特征加或减去一个数而改变。
6. 当数据有一个 0 均值向量时，PCA 有与 SVD 一样的投射，否则在使用 SVD 之前，你必须将数据均值归 0。
7. 由信息量公式 I(X) = - log[p(X)] 知，概率越大，信息量越小
8. 主成分分析是特征转换算法（特征抽取），而不是特征选择
9. 语言模型的参数估计经常使用MLE（最大似然估计）。面临的一个问题是没有出现的项概率为0，这样会导致语言模型的效果不好。为了解决这个问题，需要使用拉普拉斯平滑假设，将分子和分母各加上一个常数项。
10. SVM的决策边界只会被支持向量影响，跟其他点无关。
11. SVM的效率依赖于核函数的选择、核参数、软间隔参数
12. 对于SVM，γ 越大，模型对训练数据的拟合效果越好，当 γ 很大时，模型会对数据过拟合，即：分类超平面波动较大，几乎是贴着训练数据的
13. 