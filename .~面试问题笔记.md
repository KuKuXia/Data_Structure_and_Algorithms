# 面试问题错题集

## 生成式和判别式模型

### 常见的判别式模型有：

- Logistic regression（logistical 回归）
- Linear discriminant analysis（线性判别分析）
- Supportvector machines（支持向量机）
- Boosting（集成学习）
- Conditional random fields（条件随机场）
- Linear regression（线性回归）
- Neural networks（神经网络）

### 常见的生成式模型有:
- Gaussian mixture model and othertypes of mixture model（高斯混合及其他类型混合模型）
- Hidden Markov model（隐马尔可夫）
- NaiveBayes（朴素贝叶斯）
- AODE（平均单依赖估计）
- Latent Dirichlet allocation（LDA主题模型）
- Restricted Boltzmann Machine（限制波兹曼机）
  **生成式模型是根据概率乘出结果，而判别式模型是给出输入，计算出结果**

### 时间序列预测模型

- AR模型是一种线性预测，即已知N个数据，可由模型推出第N点前面或后面的数据（设推出P点），所以其本质类似于插值。
- MA模型(moving average model)滑动平均模型，其中使用趋势移动平均法建立直线趋势的预测模型。
  ARMA模型(auto regressive moving average model)自回归滑动平均模型，模型参量法高分辨率谱分析方法之一。这种方法是研究平稳随机过程有理谱的典型方法。它比AR模型法与MA模型法有较精确的谱估计及较优良的谱分辨率性能，但其参数估算比较繁琐。
- GARCH模型称为广义ARCH模型，是ARCH模型的拓展，由Bollerslev(1986)发展起来的。它是ARCH模型的推广。GARCH(p,0)模型，相当于ARCH(p)模型。GARCH模型是一个专门针对金融数据所量体订做的回归模型，除去和普通回归模型相同的之处，GARCH对误差的方差进行了进一步的建模。特别适用于波动性的分析和预测，这样的分析对投资者的决策能起到非常重要的指导性作用，其意义很多时候超过了对数值本身的分析和预测

### 常用特征选择算法

常采用特征选择方法。常见的六种特征选择方法：

1. DF(Document Frequency) 文档频率
   DF:统计特征词出现的文档数量，用来衡量某个特征词的重要性
2. MI(Mutual Information) 互信息法
   互信息法用于衡量特征词与文档类别直接的信息量。
   如果某个特征词的频率很低，那么互信息得分就会很大，因此互信息法倾向"低频"的特征词。
   相对的词频很高的词，得分就会变低，如果这词携带了很高的信息量，互信息法就会变得低效。
3. (Information Gain) 信息增益法
   通过某个特征词的缺失与存在的两种情况下，语料中前后信息的增加，衡量某个特征词的重要性。
4. CHI(Chi-square) 卡方检验法
   利用了统计学中的"假设检验"的基本思想：首先假设特征词与类别直接是不相关的
   如果利用CHI分布计算出的检验值偏离阈值越大，那么更有信心否定原假设，接受原假设的备则假设：特征词与类别有着很高的关联度。
5. WLLR(Weighted Log Likelihood Ration)加权对数似然
6. WFO（Weighted Frequency and Odds）加权频率和可能性

### CRF与HMM和MEMM模型

1. CRF 的优点：特征灵活，可以容纳较多的上下文信息，能够做到全局最优CRF 的缺点：速度慢
2. CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息。特征设计灵活（与ME一样） ————与HMM比较
3. 同时，由于CRF计算全局最优输出节点的条件概率，它还克服了最大熵马尔可夫模型标记偏置（Label-bias）的缺点。 ­­————与MEMM比较
4. CRF是在给定需要标记的观察序列的条件下，使用维特比算法，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。————与ME比较

### R-square与Adjusted R-squared

1. 线性回归中，R-squared描述的是输入变量（特征）对输出变量的解释程度。在单变量线性回归中R-squared 越大，说明拟合程度越好；而在多变量的情况下，无论增加的特征与输出是否存在关系（即是否重要），R-squared 要么保持不变，要么增加。故本题中可能的选项只有A。（本题中增加一个特征后至少有两个特征，所欲属于多特征范畴）

2. 多变量线性回归使用adjusted R-squared评估模型效果。并且增加一个特征变量，如果这个特征有意义，Adjusted R-Square 就会增大，若这个特征是冗余特征，Adjusted R-Squared 就会减小。

3. 单变量线性回归中，R-squared和adjusted R-squared是一致的，即重要特征使R-squared增大，冗余特征使R-squared减小。

### 数据清理中，处理缺失值的方法

据清理中，处理缺失值的方法有两种：

#### 删除法

1. 删除观察样法
2. 删除变量：当某个变量缺失值较多且对研究目标影响不大时，可以将整个变量整体删除
3. 使用完整原始数据分析：当数据存在较多缺失而其原始数据完整时，可以使用原始数据替代现有数据进行分析
4. 改变权重：当删除缺失数据会改变数据结构时，通过对完整数据按照不同的权重进行加权，可以降低删除缺失数据带来的偏差

#### 查补法

1. 均值插补
2. 回归插补
3. 抽样填补等

### Note

1. 朴素贝叶斯的条件就是每隔变量相互独立
2. L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso（L1）在特征选择时候非常有用，而Ridge（L2）就只是一种规则化而已。
3. Pearson相关系数只能衡量线性相关性，但无法衡量非线性关系。如y=x^2，x和y有很强的非线性关系。
4. boostrap是提鞋自举的意思(武侠小说作者所说的左脚踩右脚腾空而起). 它的过程是对样本(而不是特征)进行有放回的抽样, 抽样次数等同于样本总数. 这个随机抽样过程决定了最终抽样出来的样本, 去除重复之后, 占据原有样本的1/e比例. 增加树的深度, 会导致所有节点不断分裂, 直到叶子节点是纯的为止. 所以, 增加深度, 会延长训练时间.
   决策树没有学习率参数可以调. (不像集成学习和其它有步长的学习方法)
   决策树只有一棵树, 不是随机森林。
5. 特征之间的相关性系数不会因为特征加或减去一个数而改变。
6. 当数据有一个 0 均值向量时，PCA 有与 SVD 一样的投射，否则在使用 SVD 之前，你必须将数据均值归 0。
7. 由信息量公式 I(X) = - log[p(X)] 知，概率越大，信息量越小
8. 主成分分析是特征转换算法（特征抽取），而不是特征选择
9. 语言模型的参数估计经常使用MLE（最大似然估计）。面临的一个问题是没有出现的项概率为0，这样会导致语言模型的效果不好。为了解决这个问题，需要使用拉普拉斯平滑假设，将分子和分母各加上一个常数项。
10. SVM的决策边界只会被支持向量影响，跟其他点无关。
11. SVM的效率依赖于核函数的选择、核参数、软间隔参数
12. 对于SVM，γ 越大，模型对训练数据的拟合效果越好，当 γ 很大时，模型会对数据过拟合，即：分类超平面波动较大，几乎是贴着训练数据的
13. C >0称为惩罚参数，是调和二者的系数，C值大时对误差分类的惩罚增大，当C越大，趋近无穷的时候，表示不允许分类误差的存在，margin越小，容易过拟合。C值小时对误差分类的惩罚减小,当C趋于0时，表示我们不再关注分类是否正确，只要求margin越大，容易欠拟合。

14. 进程通信的几大方法中三个，分别是管道( pipe )、套接字( socket )、共享内存( shared memory)。
15. t-SNE代表t分布随机相邻嵌入，它考虑最近的邻居来减少数据。
16. SNE（随机邻域嵌入）将数据点之间高维的欧氏距离转换为表示相似度的条件概率，t-SNE重点解决了两个问题，一个是SNE的不对称问题（用联合概率分布替代条件概率分布），另一个是不同类别之间簇的拥挤问题（引入t分布）。机器学习可以总结为学习一个函数模型，它将输入变量X映射为输出变量Y，算法从训练数据中学习这个映射函数，如果事先假设了模型参数和目标函数，那么这样的机器学习算法就是在学习一种参数映射，所以t-SNE学习非参数映射。
17. 降维过程中，两点的相似性的条件概率必须相等，因为点之间的相似性必须在高维和低维中保持不变，以使它们成为完美的表示。
18. LDA在样本分类信息依赖方差而不是均值的时候，降维效果不好；LDA是有监督学习，它既可以用作数据降维，又可以用于分类，但要保证不同类别数据的投影中心尽可能远；有辨识的信息即分类依据，如果有辨识的信息不是平均值，那么就无法保证投影后的异类数据点中心尽可能远，LDA就会失败。
19. LDA最多产生c-1个判别向量。
20. AIC信息准则即Akaike information criterion，是衡量统计模型拟合优良性的一种标准，由于它为日本统计学家赤池弘次创立和发展的，因此又称赤池信息量准则。考虑到AIC=2k-2In(L) ，所以一般而言，当模型复杂度提高（k增大）时，似然函数L也会增大，从而使AIC变小，但是k过大时，似然函数增速减缓，导致AIC增大，模型过于复杂容易造成过拟合现象。目标是选取AIC最小的模型，AIC不仅要提高模型拟合度（极大似然），而且引入了惩罚项，使模型参数尽可能少，有助于降低过拟合的可能性。综上，我们一般选择逻辑回归中最少的AIC作为最佳模型。
21. 输出特征图尺寸=（N-F+2P）/ S + 1，其中N为输入图像的宽和高，F为卷积核的宽和高，P为填充值padding，S为卷积核滑动步长stride。
22. 线性回归有变化的误差项，在误差项中，非恒定方差的存在导致了异方差性。一般来说，非恒定方差的出现时因为异常值或极端杠杆值的存在。
23. 一般将残差想作垂直坐标，正交坐标在PCA的例子中很有用
24. 线性回归中，大特征值=⇒小相关系数=⇒更少lasso penalty =⇒更可能被保留
25. Ridge回归在最终模型中用到了所有自变量，然而Lasso回归可被用于特征值选择，因为相关系数可以为零
26. 异常值是指样本中的个别值明显偏离其余观测值，也叫离群值。目前人们对异常值的判别与剔除主要采用物理判别法和统计判别法两种方法。所谓物理判别法就是根据人们对客观事物已有的认识，判别由于外界干扰、人为误差等原因造成实测数据值偏离正常结果，在实验过程中随时判断，随时剔除。统计判别法是给定一个置信概率，并确定一个置信限，凡超过此限的误差，就认为它不属于随机误差范围，将其视为异常值剔除。当物理识别不易判断时，一般采用统计识别法。该题中，所给的信息量过少，无法肯定一定是异常值。
27. 使用 L1 正则化的模型叫做 Lasson 回归，使用 L2 正则化的模型叫做 Ridge 回归（岭回归）。L1正则可以保证模型的稀疏性，也就是某些参数等于0；L2正则可以保证模型的稳定性，也就是参数的值不会太大或太小。L1正则和L2正则都可以减小模型复杂度，防止过拟合。
    1. Lambda很大表示模型比较简单，这种情况下模型预测值与实际值差距较大，即偏差大，而预测值的变化范围较小，即方差小。
    2. 当lambda为0时我们得到了最小的最小二乘解；当lambda趋近无穷时，会得到非常小、趋近0的相关系数。
28. 预测值与残差之间不应该存在任何函数关系，若存在函数关系，表明模型拟合的效果并不很好。
29. Lasso不允许闭式解，L1-penalty使解为非线性的，所以需要近似解。
30. 逻辑回归的输出是因变量Y取值的概率，用于分类问题，而不是对因变量的取值进行预测，而线性回归的输出是模型对因变量Y的取值进行预测和估计。
31. 决策树还可以用在数据中的聚类分析，但是聚类常常生成自然集群，并且不依赖于任何目标函数。x例如 基于最大似然（ML）准则的决策树聚类 以及 基于最小描述长度（MDL）的决策树聚类
32. 使用信息增益作为决策树节点属性选择的标准，由于信息增益在类别值多的属性上计算结果大于类别值少的属性上计算结果，这将导致决策树算法偏向选择具有较多分枝的属性。